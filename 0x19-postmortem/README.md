*MY FIRST POSTMORTEM*
![image](https://user-images.githubusercontent.com/107412228/221952383-9b381153-5a26-4cd7-b35d-d0a3051b1fb1.png)




*Server Error Information*

On February 22, 2023, our Hospital Management System website experienced a total meltdown that left our users feeling as frustrated as a cat trying to catch a laser pointer. The meltdown occurred between the hours of 9:00 PM and 11:30 PM EST, affecting 61% of our users. During this time, users were unable to browse the website, manage inventory and the patients could not access their medical information. As a result, we experienced a 40% cut in sales and overall client reviews and our CEO's hairline receded by an inch.

*Timeline*

9:00 PM: Our monitoring system alerted us to increased error rates and server latency. Our engineering team frantically searched for the root cause, but it was as elusive as a greased pig in a mud pit.
9:05 PM: We realized we were in deep trouble when our CEO called and demanded to know what was going on. We quickly blamed the issue on aliens.
9:20 PM: After ruling out the alien theory, we suspected a server overload due to increased traffic. We tried to bribe our users with virtual cookies to get them to leave the website, but it didn't work.
9:40 PM: We then focused our attention on the database servers and discovered that one of them had a hardware failure. We tried to revive it by giving it CPR, but it was too late.
10:00 PM: We escalated the issue to the database administration team, who quickly diagnosed the issue as a corrupt database file. We swore at the database and threatened to replace it with a filing cabinet.
10:30 PM: The database administration team began restoring from a backup. We took a break to play a game of ping pong to release some stress.
11:00 PM: The database was successfully restored, and we felt a glimmer of hope. We celebrated with a virtual happy hour and drank until we forgot about the incident.
11:30 PM: The website was fully restored, and we were finally able to breathe a sigh of relief. Our CEO's hairline even grew back a millimeter.

*Root Cause and Resolution*

The root cause of the issue was a corrupt database file that caused the primary database server to fail. The database administration team resolved the issue by restoring from a backup. We promised to take better care of our database and provide it with regular checkups and emotional support.


*Corrective and Preventative Measures*

To prevent similar issues in the future, we will be implementing the following measures:
Improve database monitoring to detect and alert us of potential issues before they become critical.
Implement a more robust backup and disaster recovery plan to ensure minimal downtime in the event of a database failure.
Perform regular database maintenance to prevent corruption and optimize performance.
Train our staff on how to perform CPR on a server in case of emergency.
Host a company-wide ping pong tournament to promote stress relief and team bonding.





# MAKOHA DHARREN PIUS - AUTHOR #





